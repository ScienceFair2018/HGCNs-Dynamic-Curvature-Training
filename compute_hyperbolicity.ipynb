{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install geoopt"
      ],
      "metadata": {
        "id": "Wat_FfGGuI99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "from itertools import combinations\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm import tqdm\n",
        "\n",
        "def compute_hyperbolicity(G, sample_size=None, n_jobs=-1):\n",
        "    \"\"\"\n",
        "    Compute the hyperbolicity of a graph G.\n",
        "    Hyperbolicity is calculated based on the Gromov's four-point condition.\n",
        "\n",
        "    Parameters:\n",
        "        G (networkx.Graph): The input graph (can be disconnected).\n",
        "        sample_size (int, optional): Number of random quadruples to sample. If None, evaluates all quadruples.\n",
        "        n_jobs (int, optional): Number of parallel jobs. Defaults to -1 (use all available cores).\n",
        "\n",
        "    Returns:\n",
        "        float: The hyperbolicity of the graph.\n",
        "    \"\"\"\n",
        "    import random\n",
        "\n",
        "    # Initialize hyperbolicity\n",
        "    delta = 0\n",
        "\n",
        "    # Identify connected components\n",
        "    components = list(nx.connected_components(G))\n",
        "\n",
        "    # Get all nodes\n",
        "    nodes = list(G.nodes())\n",
        "\n",
        "    # Generate quadruples\n",
        "    if sample_size is None:\n",
        "        quadruples = list(combinations(nodes, 4))\n",
        "    else:\n",
        "        quadruples = [tuple(random.sample(nodes, 4)) for _ in range(sample_size)]\n",
        "\n",
        "    def process_quadruple(quad):\n",
        "        u, v, x, y = quad\n",
        "\n",
        "        # Check if all nodes belong to the same connected component\n",
        "        component = next((comp for comp in components if {u, v, x, y}.issubset(comp)), None)\n",
        "        if component is None:\n",
        "            return 0\n",
        "\n",
        "        # Compute shortest paths dynamically\n",
        "        try:\n",
        "            subgraph = G.subgraph(component)\n",
        "            d_uv = nx.shortest_path_length(subgraph, source=u, target=v)\n",
        "            d_ux = nx.shortest_path_length(subgraph, source=u, target=x)\n",
        "            d_uy = nx.shortest_path_length(subgraph, source=u, target=y)\n",
        "            d_vx = nx.shortest_path_length(subgraph, source=v, target=x)\n",
        "            d_vy = nx.shortest_path_length(subgraph, source=v, target=y)\n",
        "            d_xy = nx.shortest_path_length(subgraph, source=x, target=y)\n",
        "        except nx.NetworkXNoPath:\n",
        "            return 0\n",
        "\n",
        "        # Compute the three sums\n",
        "        s1 = d_uv + d_xy\n",
        "        s2 = d_ux + d_vy\n",
        "        s3 = d_uy + d_vx\n",
        "\n",
        "        # Compute the maximum of these sums\n",
        "        max_s = max(s1, s2, s3)\n",
        "\n",
        "        # Compute the hyperbolicity for this quadruple\n",
        "        delta_quad = (max_s - min(s1, s2, s3)) / 2\n",
        "\n",
        "        return delta_quad\n",
        "\n",
        "    # Process quadruples in parallel\n",
        "    results = Parallel(n_jobs=n_jobs)(delayed(process_quadruple)(quad) for quad in tqdm(quadruples, desc=\"Processing quadruples\"))\n",
        "\n",
        "    # Compute final hyperbolicity\n",
        "    delta = max(results)\n",
        "\n",
        "    return delta"
      ],
      "metadata": {
        "id": "dcrPU9FJuGe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpWVs5tXt__8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "from torch import optim\n",
        "import geoopt\n",
        "from geoopt import ManifoldParameter\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "##############################################\n",
        "# Data Loading and Preprocessing\n",
        "##############################################\n",
        "\n",
        "def load_fb15k_data(train_path, valid_path, test_path):\n",
        "    def read_triples(path, ent2id, rel2id):\n",
        "        triples = []\n",
        "        df = pd.read_csv(path, header=None, names=[\"head\", \"relation\", \"tail\"])\n",
        "        for _, row in df.iterrows():\n",
        "            h, r, t = row[\"head\"], row[\"relation\"], row[\"tail\"]\n",
        "            if h not in ent2id:\n",
        "                ent2id[h] = len(ent2id)\n",
        "            if t not in ent2id:\n",
        "                ent2id[t] = len(ent2id)\n",
        "            if r not in rel2id:\n",
        "                rel2id[r] = len(rel2id)\n",
        "            triples.append((ent2id[h], rel2id[r], ent2id[t]))\n",
        "        return triples\n",
        "\n",
        "    ent2id = {}\n",
        "    rel2id = {}\n",
        "    train_triples = read_triples(train_path, ent2id, rel2id)\n",
        "    valid_triples = read_triples(valid_path, ent2id, rel2id)\n",
        "    test_triples = read_triples(test_path, ent2id, rel2id)\n",
        "\n",
        "    G = nx.Graph()\n",
        "    G.add_nodes_from(range(len(ent2id)))\n",
        "    for h, r, t in train_triples:\n",
        "        G.add_edge(h, t)\n",
        "\n",
        "    return ent2id, rel2id, G, train_triples, valid_triples, test_triples\n",
        "\n",
        "# Define file paths\n",
        "train_path = 'Wordnet_train.csv'\n",
        "valid_path = 'Wordnet_valid.csv'\n",
        "test_path = 'Wordnet_test.csv'\n",
        "\n",
        "# Load data\n",
        "ent2id, rel2id, G, train_triples, valid_triples, test_triples = load_fb15k_data(train_path, valid_path, test_path)\n",
        "\n",
        "num_entities = len(ent2id)\n",
        "num_relations = len(rel2id)\n",
        "\n",
        "print(f\"Number of Entities: {num_entities}\")\n",
        "print(f\"Number of Relations: {num_relations}\")\n",
        "print(f\"Number of Training Triples: {len(train_triples)}\")\n",
        "print(f\"Number of Validation Triples: {len(valid_triples)}\")\n",
        "print(f\"Number of Test Triples: {len(test_triples)}\")\n",
        "\n",
        "from scipy.sparse import csr_matrix, diags\n",
        "import numpy as np\n",
        "\n",
        "# Convert adjacency matrix to a sparse matrix\n",
        "adj = nx.to_scipy_sparse_array(G, nodelist=range(num_entities)).tocsc()\n",
        "\n",
        "# Add self-loops\n",
        "adj += diags([1e-5] * num_entities)\n",
        "\n",
        "# Compute degree\n",
        "degrees = np.array(adj.sum(axis=1)).flatten()\n",
        "\n",
        "# Compute D^-0.5 (inverse square root of degree matrix)\n",
        "inv_sqrt_deg = np.power(degrees, -0.5)\n",
        "inv_sqrt_deg[np.isinf(inv_sqrt_deg)] = 0\n",
        "D_inv_sqrt = diags(inv_sqrt_deg)\n",
        "\n",
        "# Normalize adjacency matrix: D^-0.5 * A * D^-0.5\n",
        "norm_adj = D_inv_sqrt @ adj @ D_inv_sqrt\n",
        "\n",
        "# Convert back to a PyTorch tensor\n",
        "# To save memory, keep it on CPU if possible or use sparse tensors\n",
        "norm_adj = torch.tensor(norm_adj.toarray(), dtype=torch.float32)  # If GPU is available, use .to(device)\n",
        "\n",
        "# Node Features: Use lower-dimensional learnable embeddings instead of one-hot\n",
        "embedding_dim = 128  # Reduced dimensionality\n",
        "# Initialize features as learnable parameters\n",
        "features = nn.Parameter(torch.randn(num_entities, embedding_dim), requires_grad=True)  # Initialize on CPU\n",
        "\n",
        "# Move tensors to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "features, norm_adj = features.to(device), norm_adj.to(device)\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "print(\"Hyperbolicity:\", compute_hyperbolicity(G, sample_size=10000))"
      ]
    }
  ]
}