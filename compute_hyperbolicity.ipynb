{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install geoopt"
      ],
      "metadata": {
        "id": "Wat_FfGGuI99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a9d0785-0400-4870-8e43-66d720769b09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting geoopt\n",
            "  Downloading geoopt-0.5.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from geoopt) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from geoopt) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from geoopt) (1.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.9.0->geoopt) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->geoopt) (3.0.2)\n",
            "Downloading geoopt-0.5.0-py3-none-any.whl (90 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.1/90.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: geoopt\n",
            "Successfully installed geoopt-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "from itertools import combinations\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm import tqdm\n",
        "\n",
        "def compute_hyperbolicity(G, sample_size=None, n_jobs=-1):\n",
        "    \"\"\"\n",
        "    Compute the hyperbolicity of a graph G.\n",
        "    Hyperbolicity is calculated based on the Gromov's four-point condition.\n",
        "\n",
        "    Parameters:\n",
        "        G (networkx.Graph): The input graph (can be disconnected).\n",
        "        sample_size (int, optional): Number of random quadruples to sample. If None, evaluates all quadruples.\n",
        "        n_jobs (int, optional): Number of parallel jobs. Defaults to -1 (use all available cores).\n",
        "\n",
        "    Returns:\n",
        "        float: The hyperbolicity of the graph.\n",
        "    \"\"\"\n",
        "    import random\n",
        "    random.seed(88)\n",
        "\n",
        "    # Initialize hyperbolicity\n",
        "    delta = 0\n",
        "\n",
        "    # Identify connected components\n",
        "    components = list(nx.connected_components(G))\n",
        "\n",
        "    # Get all nodes\n",
        "    nodes = list(G.nodes())\n",
        "\n",
        "    # Generate quadruples\n",
        "    if sample_size is None:\n",
        "        quadruples = list(combinations(nodes, 4))\n",
        "    else:\n",
        "        quadruples = [tuple(random.sample(nodes, 4)) for _ in range(sample_size)]\n",
        "\n",
        "    def process_quadruple(quad):\n",
        "        u, v, x, y = quad\n",
        "\n",
        "        # Check if all nodes belong to the same connected component\n",
        "        component = next((comp for comp in components if {u, v, x, y}.issubset(comp)), None)\n",
        "        if component is None:\n",
        "            return 0\n",
        "\n",
        "        # Compute shortest paths dynamically\n",
        "        try:\n",
        "            subgraph = G.subgraph(component)\n",
        "            d_uv = nx.shortest_path_length(subgraph, source=u, target=v)\n",
        "            d_ux = nx.shortest_path_length(subgraph, source=u, target=x)\n",
        "            d_uy = nx.shortest_path_length(subgraph, source=u, target=y)\n",
        "            d_vx = nx.shortest_path_length(subgraph, source=v, target=x)\n",
        "            d_vy = nx.shortest_path_length(subgraph, source=v, target=y)\n",
        "            d_xy = nx.shortest_path_length(subgraph, source=x, target=y)\n",
        "        except nx.NetworkXNoPath:\n",
        "            return 0\n",
        "\n",
        "        # Compute the three sums\n",
        "        s1 = d_uv + d_xy\n",
        "        s2 = d_ux + d_vy\n",
        "        s3 = d_uy + d_vx\n",
        "\n",
        "        # Compute the maximum of these sums\n",
        "        max_s = max(s1, s2, s3)\n",
        "\n",
        "        # Compute the hyperbolicity for this quadruple\n",
        "        delta_quad = (max_s - min(s1, s2, s3)) / 2\n",
        "\n",
        "        return delta_quad\n",
        "\n",
        "    # Process quadruples in parallel\n",
        "    results = Parallel(n_jobs=n_jobs)(delayed(process_quadruple)(quad) for quad in tqdm(quadruples, desc=\"Processing quadruples\"))\n",
        "\n",
        "    # Compute final hyperbolicity\n",
        "    delta = max(results)\n",
        "\n",
        "    return delta"
      ],
      "metadata": {
        "id": "dcrPU9FJuGe_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KpWVs5tXt__8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "684758d1-a99d-43db-e078-67978ce972c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Entities: 14541\n",
            "Number of Relations: 237\n",
            "Number of Training Triples: 272115\n",
            "Number of Validation Triples: 17535\n",
            "Number of Test Triples: 20466\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "Processing quadruples:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:   1%|          | 1/100 [00:00<00:11,  8.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:   4%|▍         | 4/100 [00:01<00:41,  2.29it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:   6%|▌         | 6/100 [00:02<00:31,  2.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:   7%|▋         | 7/100 [00:02<00:27,  3.39it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:   8%|▊         | 8/100 [00:02<00:28,  3.21it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  10%|█         | 10/100 [00:03<00:24,  3.63it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  12%|█▏        | 12/100 [00:03<00:22,  3.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  13%|█▎        | 13/100 [00:03<00:20,  4.19it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  14%|█▍        | 14/100 [00:03<00:20,  4.19it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  16%|█▌        | 16/100 [00:04<00:19,  4.31it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  18%|█▊        | 18/100 [00:04<00:18,  4.46it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  20%|██        | 20/100 [00:05<00:18,  4.38it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  22%|██▏       | 22/100 [00:05<00:20,  3.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  23%|██▎       | 23/100 [00:06<00:21,  3.57it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  24%|██▍       | 24/100 [00:06<00:21,  3.56it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  26%|██▌       | 26/100 [00:07<00:23,  3.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  28%|██▊       | 28/100 [00:08<00:24,  2.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  30%|███       | 30/100 [00:08<00:23,  3.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  32%|███▏      | 32/100 [00:09<00:20,  3.40it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  34%|███▍      | 34/100 [00:09<00:18,  3.64it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  36%|███▌      | 36/100 [00:10<00:16,  3.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  38%|███▊      | 38/100 [00:10<00:14,  4.19it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  40%|████      | 40/100 [00:10<00:13,  4.33it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  42%|████▏     | 42/100 [00:11<00:12,  4.49it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  44%|████▍     | 44/100 [00:11<00:12,  4.51it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  46%|████▌     | 46/100 [00:12<00:11,  4.61it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  48%|████▊     | 48/100 [00:12<00:11,  4.57it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  50%|█████     | 50/100 [00:12<00:10,  4.64it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  52%|█████▏    | 52/100 [00:13<00:10,  4.52it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  53%|█████▎    | 53/100 [00:13<00:10,  4.64it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  54%|█████▍    | 54/100 [00:13<00:09,  4.77it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  55%|█████▌    | 55/100 [00:13<00:09,  4.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  56%|█████▌    | 56/100 [00:14<00:09,  4.62it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  58%|█████▊    | 58/100 [00:14<00:09,  4.41it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  59%|█████▉    | 59/100 [00:14<00:09,  4.49it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  60%|██████    | 60/100 [00:15<00:08,  4.64it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  62%|██████▏   | 62/100 [00:15<00:08,  4.59it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  64%|██████▍   | 64/100 [00:15<00:07,  5.11it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  65%|██████▌   | 65/100 [00:16<00:06,  5.32it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  66%|██████▌   | 66/100 [00:16<00:08,  4.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  68%|██████▊   | 68/100 [00:16<00:07,  4.11it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  70%|███████   | 70/100 [00:17<00:06,  4.54it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  72%|███████▏  | 72/100 [00:17<00:06,  4.62it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  74%|███████▍  | 74/100 [00:18<00:05,  4.45it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  76%|███████▌  | 76/100 [00:18<00:05,  4.43it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  78%|███████▊  | 78/100 [00:19<00:06,  3.54it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  80%|████████  | 80/100 [00:19<00:05,  3.80it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  82%|████████▏ | 82/100 [00:20<00:05,  3.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  84%|████████▍ | 84/100 [00:21<00:05,  2.74it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  86%|████████▌ | 86/100 [00:22<00:04,  3.16it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  88%|████████▊ | 88/100 [00:22<00:03,  3.43it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  90%|█████████ | 90/100 [00:23<00:02,  4.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  91%|█████████ | 91/100 [00:23<00:02,  4.40it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  92%|█████████▏| 92/100 [00:23<00:02,  3.68it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  94%|█████████▍| 94/100 [00:24<00:01,  3.71it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  95%|█████████▌| 95/100 [00:24<00:01,  3.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  96%|█████████▌| 96/100 [00:24<00:00,  4.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples:  98%|█████████▊| 98/100 [00:24<00:00,  4.17it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Processing quadruples: 100%|██████████| 100/100 [00:25<00:00,  3.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperbolicity: 1.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "from torch import optim\n",
        "import geoopt\n",
        "from geoopt import ManifoldParameter\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "##############################################\n",
        "# Data Loading and Preprocessing\n",
        "##############################################\n",
        "\n",
        "# def load_fb15k_data(train_path, valid_path, test_path):\n",
        "#     def read_triples(path, ent2id, rel2id):\n",
        "#         triples = []\n",
        "#         df = pd.read_csv(path, header=None, names=[\"head\", \"relation\", \"tail\"])\n",
        "#         for _, row in df.iterrows():\n",
        "#             h, r, t = row[\"head\"], row[\"relation\"], row[\"tail\"]\n",
        "#             if h not in ent2id:\n",
        "#                 ent2id[h] = len(ent2id)\n",
        "#             if t not in ent2id:\n",
        "#                 ent2id[t] = len(ent2id)\n",
        "#             if r not in rel2id:\n",
        "#                 rel2id[r] = len(rel2id)\n",
        "#             triples.append((ent2id[h], rel2id[r], ent2id[t]))\n",
        "#         return triples\n",
        "\n",
        "#     ent2id = {}\n",
        "#     rel2id = {}\n",
        "#     train_triples = read_triples(train_path, ent2id, rel2id)\n",
        "#     valid_triples = read_triples(valid_path, ent2id, rel2id)\n",
        "#     test_triples = read_triples(test_path, ent2id, rel2id)\n",
        "\n",
        "#     G = nx.Graph()\n",
        "#     G.add_nodes_from(range(len(ent2id)))\n",
        "#     for h, r, t in train_triples:\n",
        "#         G.add_edge(h, t)\n",
        "\n",
        "#     return ent2id, rel2id, G, train_triples, valid_triples, test_triples\n",
        "\n",
        "\n",
        "def load_fb15k_data(train_path, valid_path, test_path):\n",
        "    def read_triples(path, ent2id, rel2id):\n",
        "        triples = []\n",
        "        with open(path, 'r') as f:\n",
        "            for line in f:\n",
        "                h, r, t = line.strip().split('\\t')\n",
        "                if h not in ent2id:\n",
        "                    ent2id[h] = len(ent2id)\n",
        "                if t not in ent2id:\n",
        "                    ent2id[t] = len(ent2id)\n",
        "                if r not in rel2id:\n",
        "                    rel2id[r] = len(rel2id)\n",
        "                triples.append((ent2id[h], rel2id[r], ent2id[t]))\n",
        "        return triples\n",
        "\n",
        "    ent2id = {}\n",
        "    rel2id = {}\n",
        "    train_triples = read_triples(train_path, ent2id, rel2id)\n",
        "    valid_triples = read_triples(valid_path, ent2id, rel2id)\n",
        "    test_triples = read_triples(test_path, ent2id, rel2id)\n",
        "\n",
        "    G = nx.Graph()\n",
        "    G.add_nodes_from(range(len(ent2id)))\n",
        "    for h, r, t in train_triples:\n",
        "        G.add_edge(h, t)\n",
        "\n",
        "    return ent2id, rel2id, G, train_triples, valid_triples, test_triples\n",
        "\n",
        "# Define file paths\n",
        "# train_path = 'Wordnet_train.csv'\n",
        "# valid_path = 'Wordnet_valid.csv'\n",
        "# test_path = 'Wordnet_test.csv'\n",
        "\n",
        "\n",
        "train_path = 'FB15k_train.txt'\n",
        "valid_path = 'FB15k_valid.txt'\n",
        "test_path = 'FB15k_test.txt'\n",
        "\n",
        "\n",
        "# Load data\n",
        "ent2id, rel2id, G, train_triples, valid_triples, test_triples = load_fb15k_data(train_path, valid_path, test_path)\n",
        "\n",
        "num_entities = len(ent2id)\n",
        "num_relations = len(rel2id)\n",
        "\n",
        "print(f\"Number of Entities: {num_entities}\")\n",
        "print(f\"Number of Relations: {num_relations}\")\n",
        "print(f\"Number of Training Triples: {len(train_triples)}\")\n",
        "print(f\"Number of Validation Triples: {len(valid_triples)}\")\n",
        "print(f\"Number of Test Triples: {len(test_triples)}\")\n",
        "\n",
        "from scipy.sparse import csr_matrix, diags\n",
        "import numpy as np\n",
        "\n",
        "# Convert adjacency matrix to a sparse matrix\n",
        "adj = nx.to_scipy_sparse_array(G, nodelist=range(num_entities)).tocsc()\n",
        "\n",
        "# Add self-loops\n",
        "adj += diags([1e-5] * num_entities)\n",
        "\n",
        "# Compute degree\n",
        "degrees = np.array(adj.sum(axis=1)).flatten()\n",
        "\n",
        "# Compute D^-0.5 (inverse square root of degree matrix)\n",
        "inv_sqrt_deg = np.power(degrees, -0.5)\n",
        "inv_sqrt_deg[np.isinf(inv_sqrt_deg)] = 0\n",
        "D_inv_sqrt = diags(inv_sqrt_deg)\n",
        "\n",
        "# Normalize adjacency matrix: D^-0.5 * A * D^-0.5\n",
        "norm_adj = D_inv_sqrt @ adj @ D_inv_sqrt\n",
        "\n",
        "# Convert back to a PyTorch tensor\n",
        "# To save memory, keep it on CPU if possible or use sparse tensors\n",
        "norm_adj = torch.tensor(norm_adj.toarray(), dtype=torch.float32)  # If GPU is available, use .to(device)\n",
        "\n",
        "# Node Features: Use lower-dimensional learnable embeddings instead of one-hot\n",
        "embedding_dim = 128  # Reduced dimensionality\n",
        "# Initialize features as learnable parameters\n",
        "features = nn.Parameter(torch.randn(num_entities, embedding_dim), requires_grad=True)  # Initialize on CPU\n",
        "\n",
        "# Move tensors to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "features, norm_adj = features.to(device), norm_adj.to(device)\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "print(\"Hyperbolicity:\", compute_hyperbolicity(G, sample_size=100)) # INITIAL CALCULATION FOR 1.5 DELTA WAS DONE WITH A SAMPLE SIZE OF 20000 BUT THIS SMALLER SAMPLE RUN IS DONE TO RUN IN A SHORT TIME"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INITIAL CALCULATION FOR 1.5 DELTA WAS DONE WITH A SAMPLE SIZE OF 20000 BUT THIS SMALLER SAMPLE RUN IS DONE TO RUN IN A SHORT TIME"
      ],
      "metadata": {
        "id": "84x7d5lzHufU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pDWdDrx-G9AG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}