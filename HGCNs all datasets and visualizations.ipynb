{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYiEJmccXcZz"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install geoopt\n",
        "!pip install networkx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WORDNET and FB15k"
      ],
      "metadata": {
        "id": "uCwSPeby9PzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "from torch import optim\n",
        "import geoopt\n",
        "from geoopt import ManifoldParameter\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "#change here\n",
        "dataset = 'fb15k' # replace with wordnet\n",
        "\n",
        "\n",
        "\n",
        "##############################################\n",
        "# Data Loading and Preprocessing\n",
        "##############################################\n",
        "def load_fb15k_data(train_path, valid_path, test_path):\n",
        "    def read_triples(path, ent2id, rel2id):\n",
        "        triples = []\n",
        "        with open(path, 'r') as f:\n",
        "            for line in f:\n",
        "                h, r, t = line.strip().split('\\t')\n",
        "                if h not in ent2id:\n",
        "                    ent2id[h] = len(ent2id)\n",
        "                if t not in ent2id:\n",
        "                    ent2id[t] = len(ent2id)\n",
        "                if r not in rel2id:\n",
        "                    rel2id[r] = len(rel2id)\n",
        "                triples.append((ent2id[h], rel2id[r], ent2id[t]))\n",
        "        return triples\n",
        "\n",
        "    ent2id = {}\n",
        "    rel2id = {}\n",
        "    train_triples = read_triples(train_path, ent2id, rel2id)\n",
        "    valid_triples = read_triples(valid_path, ent2id, rel2id)\n",
        "    test_triples = read_triples(test_path, ent2id, rel2id)\n",
        "\n",
        "    G = nx.Graph()\n",
        "    G.add_nodes_from(range(len(ent2id)))\n",
        "    for h, r, t in train_triples:\n",
        "        G.add_edge(h, t)\n",
        "\n",
        "    return ent2id, rel2id, G, train_triples, valid_triples, test_triples\n",
        "\n",
        "def load_wordnet_data(train_path, valid_path, test_path):\n",
        "    def read_triples(path, ent2id, rel2id):\n",
        "        triples = []\n",
        "        df = pd.read_csv(path, header=None, names=[\"head\", \"relation\", \"tail\"])\n",
        "        for _, row in df.iterrows():\n",
        "            h, r, t = row[\"head\"], row[\"relation\"], row[\"tail\"]\n",
        "            if h not in ent2id:\n",
        "                ent2id[h] = len(ent2id)\n",
        "            if t not in ent2id:\n",
        "                ent2id[t] = len(ent2id)\n",
        "            if r not in rel2id:\n",
        "                rel2id[r] = len(rel2id)\n",
        "            triples.append((ent2id[h], rel2id[r], ent2id[t]))\n",
        "        return triples\n",
        "\n",
        "    ent2id = {}\n",
        "    rel2id = {}\n",
        "    train_triples = read_triples(train_path, ent2id, rel2id)\n",
        "    valid_triples = read_triples(valid_path, ent2id, rel2id)\n",
        "    test_triples = read_triples(test_path, ent2id, rel2id)\n",
        "\n",
        "    G = nx.Graph()\n",
        "    G.add_nodes_from(range(len(ent2id)))\n",
        "    for h, r, t in train_triples:\n",
        "        G.add_edge(h, t)\n",
        "\n",
        "    return ent2id, rel2id, G, train_triples, valid_triples, test_triples\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load data\n",
        "if dataset == 'fb15k':\n",
        "    train_path = 'FB15k_train.txt'\n",
        "    valid_path = 'FB15k_valid.txt'\n",
        "    test_path = 'FB15k_test.txt'\n",
        "    ent2id, rel2id, G, train_triples, valid_triples, test_triples = load_fb15k_data(train_path, valid_path, test_path) # fb15k\n",
        "\n",
        "elif dataset == 'wordnet':\n",
        "    train_path = 'wordnet_train.csv'\n",
        "    valid_path = 'wordnet_valid.csv'\n",
        "    test_path = 'wordnet_test.csv'\n",
        "    ent2id, rel2id, G, train_triples, valid_triples, test_triples = load_wordnet_data(train_path, valid_path, test_path) # wordnet\n",
        "\n",
        "\n",
        "num_entities = len(ent2id)\n",
        "num_relations = len(rel2id)\n",
        "\n",
        "print(f\"Number of Entities: {num_entities}\")\n",
        "print(f\"Number of Relations: {num_relations}\")\n",
        "print(f\"Number of Training Triples: {len(train_triples)}\")\n",
        "print(f\"Number of Validation Triples: {len(valid_triples)}\")\n",
        "print(f\"Number of Test Triples: {len(test_triples)}\")\n",
        "\n",
        "from scipy.sparse import csr_matrix, diags\n",
        "import numpy as np\n",
        "\n",
        "# Convert adjacency matrix to a sparse matrix\n",
        "adj = nx.to_scipy_sparse_array(G, nodelist=range(num_entities)).tocsc()\n",
        "\n",
        "# Add self-loops\n",
        "adj += diags([1e-5] * num_entities)\n",
        "\n",
        "# Compute degree\n",
        "degrees = np.array(adj.sum(axis=1)).flatten()\n",
        "\n",
        "# Compute D^-0.5 (inverse square root of degree matrix)\n",
        "inv_sqrt_deg = np.power(degrees, -0.5)\n",
        "inv_sqrt_deg[np.isinf(inv_sqrt_deg)] = 0\n",
        "D_inv_sqrt = diags(inv_sqrt_deg)\n",
        "\n",
        "# Normalize adjacency matrix: D^-0.5 * A * D^-0.5\n",
        "norm_adj = D_inv_sqrt @ adj @ D_inv_sqrt\n",
        "\n",
        "# Convert back to a PyTorch tensor\n",
        "# To save memory, keep it on CPU if possible or use sparse tensors\n",
        "norm_adj = torch.tensor(norm_adj.toarray(), dtype=torch.float32)  # If GPU is available, use .to(device)\n",
        "\n",
        "# Node Features: Use lower-dimensional learnable embeddings instead of one-hot\n",
        "embedding_dim = 128  # Reduced dimensionality\n",
        "# Initialize features as learnable parameters\n",
        "features = nn.Parameter(torch.randn(num_entities, embedding_dim), requires_grad=False)  # Initialize on CPU\n",
        "\n",
        "# Move tensors to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "features, norm_adj = features.to(device), norm_adj.to(device)\n",
        "\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "id": "QJpRZR-Qxv2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################\n",
        "# Hyperbolic GCN and Link Prediction Model\n",
        "##############################################\n",
        "\n",
        "class HyperbolicLinear(nn.Module):\n",
        "    \"\"\"\n",
        "    Hyperbolic linear layer using the Poincaré Ball model.\n",
        "    \"\"\"\n",
        "    def __init__(self, manifold, in_features, out_features, bias=True):\n",
        "        super(HyperbolicLinear, self).__init__()\n",
        "        self.manifold = manifold\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        # ManifoldParameter for weight in Euclidean space\n",
        "        self.weight = ManifoldParameter(torch.randn(out_features, in_features) * 0.01, manifold=geoopt.Euclidean())\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is on manifold. Map to tangent space at 0, apply linear, map back\n",
        "        x_tan = self.manifold.logmap0(x, dim=-1)\n",
        "        out = F.linear(x_tan, self.weight, self.bias)\n",
        "        out = self.manifold.expmap0(out, dim=-1)\n",
        "        return out\n",
        "\n",
        "class HyperbolicGCNLayer(nn.Module):\n",
        "    def __init__(self, manifold, in_features, out_features):\n",
        "        super(HyperbolicGCNLayer, self).__init__()\n",
        "        self.manifold = manifold\n",
        "        self.lin = HyperbolicLinear(manifold, in_features, out_features)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        # x on manifold\n",
        "        x_tan = self.manifold.logmap0(x, dim=-1)\n",
        "        x_agg_tan = adj @ x_tan  # Aggregate in tangent space\n",
        "        x_agg = self.manifold.expmap0(x_agg_tan, dim=-1)\n",
        "        return self.lin(x_agg)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############################################################################################\n",
        "\n",
        "\n",
        "class HyperbolicGCN(nn.Module):\n",
        "    def __init__(self, num_features, hidden_dim, c=1.0):\n",
        "        super(HyperbolicGCN, self).__init__()\n",
        "        # Initialize the manifold inside the model to ensure its parameters are included\n",
        "        self.manifold = geoopt.PoincareBall(c=c, learnable=True)\n",
        "        self.layer1 = HyperbolicGCNLayer(self.manifold, num_features, hidden_dim)\n",
        "        self.layer2 = HyperbolicGCNLayer(self.manifold, hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = self.manifold.expmap0(x, dim=-1)  # Ensure x is on manifold\n",
        "        x = self.layer1(x, adj)\n",
        "        # Hyperbolic activation: apply tanh in tangent space\n",
        "        x_tan = self.manifold.logmap0(x, dim=-1)\n",
        "        x_tan = torch.tanh(x_tan)\n",
        "        x = self.manifold.expmap0(x_tan, dim=-1)\n",
        "        x = self.layer2(x, adj)\n",
        "        return x\n",
        "\n",
        "from geoopt.manifolds import PoincareBall\n",
        "\n",
        "class LinkPredictionModel(nn.Module):\n",
        "    def __init__(self, num_features, hidden_dim, num_relations, c=1.0):\n",
        "        super(LinkPredictionModel, self).__init__()\n",
        "        self.manifold = PoincareBall(c=c)\n",
        "        self.node_emb = geoopt.ManifoldParameter(\n",
        "            self.manifold.random((num_features, hidden_dim), std=1e-2),\n",
        "            manifold=self.manifold\n",
        "        )\n",
        "        self.gcn = HyperbolicGCN(num_features, hidden_dim, c=c)\n",
        "        self.classifier = nn.Linear(hidden_dim * 2, num_relations)\n",
        "        nn.init.xavier_normal_(self.classifier.weight)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        node_emb = self.gcn(x, adj)  # [num_nodes, hidden_dim]\n",
        "        return node_emb\n",
        "\n",
        "    def classify(self, head_idx, tail_idx, node_emb):\n",
        "        # Retrieve embeddings\n",
        "        head_emb = node_emb[head_idx]     # [batch_size, hidden_dim]\n",
        "        tail_emb = node_emb[tail_idx]     # [batch_size, hidden_dim]\n",
        "        # Concatenate head and tail embeddings\n",
        "        combined = torch.cat([head_emb, tail_emb], dim=1)  # [batch_size, hidden_dim * 2]\n",
        "        scores = self.classifier(combined)  # [batch_size, num_relations]\n",
        "        return scores\n",
        "\n",
        "##############################################\n",
        "# Training and Evaluation Utilities\n",
        "##############################################\n",
        "\n",
        "def prepare_data(triples):\n",
        "    \"\"\"\n",
        "    Prepare tensors for head, tail, and relation indices.\n",
        "    \"\"\"\n",
        "    triples = torch.tensor(triples, dtype=torch.long, device=device)\n",
        "    heads = triples[:, 0]\n",
        "    relations = triples[:, 1]\n",
        "    tails = triples[:, 2]\n",
        "    return heads, tails, relations\n",
        "\n",
        "def train_epoch(model, optimizer, criterion, heads, tails, relations, node_emb):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Get relation scores\n",
        "    scores = model.classify(heads, tails, node_emb)  # [batch_size, num_relations]\n",
        "\n",
        "    # Compute loss\n",
        "    loss = criterion(scores, relations)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "def evaluate(model, criterion, heads, tails, relations, node_emb):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        scores = model.classify(heads, tails, node_emb)  # [batch_size, num_relations]\n",
        "        loss = criterion(scores, relations).item()\n",
        "        preds = torch.argmax(scores, dim=1)\n",
        "        correct = (preds == relations).sum().item()\n",
        "        total = relations.size(0)\n",
        "        accuracy = correct / total\n",
        "    return loss, accuracy"
      ],
      "metadata": {
        "id": "3j8Y1qUoIeC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIMIZER CODING\n",
        "\n",
        "# Riemannian Adagrad\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "class RiemannianAdagrad(Optimizer):\n",
        "    def __init__(self, params, lr=1e-2, eps=1e-10, weight_decay=0):\n",
        "        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('RiemannianAdagrad does not support sparse gradients')\n",
        "\n",
        "                # Retrieve manifold\n",
        "                manifold = getattr(p, \"manifold\", None)\n",
        "                if manifold is None:\n",
        "                    raise RuntimeError(\"Parameter does not have a manifold defined\")\n",
        "\n",
        "                # Initialize state\n",
        "                state = self.state[p]\n",
        "                if \"sum_sq\" not in state:\n",
        "                    state[\"sum_sq\"] = torch.zeros_like(p.data)\n",
        "\n",
        "                sum_sq = state[\"sum_sq\"]\n",
        "\n",
        "                # Update sum of squared gradients\n",
        "                sum_sq.addcmul_(grad, grad, value=1)\n",
        "\n",
        "                # Apply weight decay if necessary\n",
        "                if group[\"weight_decay\"] != 0:\n",
        "                    grad = grad.add(p.data, alpha=group[\"weight_decay\"])\n",
        "\n",
        "                # Compute update\n",
        "                update = grad / (sum_sq.sqrt().add(group[\"eps\"]))\n",
        "\n",
        "                # Riemannian update\n",
        "                update = -group[\"lr\"] * update\n",
        "                p.data = manifold.proj(manifold.expmap(update, p.data))\n",
        "\n",
        "        return loss\n",
        "\n",
        "# Mixed-precision Riemannian optimizer\n",
        "from torch.cuda.amp import GradScaler\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "class MixedPrecisionRiemannianAdam(geoopt.optim.RiemannianAdam):\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"\n",
        "        Override step to use gradient scaling for mixed precision.\n",
        "        \"\"\"\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "        else:\n",
        "            loss = None\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is not None:\n",
        "                    # Scale the gradient back to normal scale\n",
        "                    p.grad.data = scaler.scale(p.grad.data)\n",
        "\n",
        "        super().step(closure)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "5FgDpNc-8kYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################ criterion = nn.CrossEntropyLoss(), Multiclass #################################\n",
        "# SET OPTIMIZER, LEARNING RATE, and C INITIAL VALUE\n",
        "\n",
        "##############################################\n",
        "# Training Loop\n",
        "##############################################\n",
        "\n",
        "# Initialize the model\n",
        "hidden_dim = 128  # Reduced hidden dimension\n",
        "model = LinkPredictionModel(num_features=embedding_dim, hidden_dim=hidden_dim, num_relations=num_relations, c=1.0).to(device)\n",
        "\n",
        "# Initialize input: ensure features are on the manifold\n",
        "with torch.no_grad():\n",
        "    x = model.gcn.manifold.expmap0(features, dim=-1)\n",
        "\n",
        "# Optimizer Choices:\n",
        "riemannian_adam = geoopt.optim.RiemannianAdam(model.parameters(), lr=1e-2)\n",
        "riemannian_sgd = geoopt.optim.RiemannianSGD(model.parameters(), lr=1e-1)\n",
        "riemannian_mixed_precision = MixedPrecisionRiemannianAdam(model.parameters(), lr=1e-1)\n",
        "riemannian_adagrad = RiemannianAdagrad(model.parameters(), lr=1e-2) # error currently\n",
        "\n",
        "# SET OPTIMIZER:\n",
        "optimizer = riemannian_mixed_precision\n",
        "\n",
        "# Define loss function for multi-class link prediction\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Prepare training, validation, and test data\n",
        "train_heads, train_tails, train_relations = prepare_data(train_triples)\n",
        "valid_heads, valid_tails, valid_relations = prepare_data(valid_triples)\n",
        "test_heads, test_tails, test_relations = prepare_data(test_triples)\n",
        "\n",
        "num_epochs = 1500\n",
        "best_val_accuracy = 0.0\n",
        "best_test_accuracy = 0.0\n",
        "\n",
        "curvatures, train_losses, val_losses, val_accuracies, test_accuracies = [], [], [], [], []\n",
        "\n",
        "print(\"\\nStarting Training...\\n\")\n",
        "for epoch in tqdm(range(1, num_epochs + 1)):\n",
        "    # Forward pass to get node embeddings\n",
        "    node_emb = model(features, norm_adj)  # [num_nodes, hidden_dim]\n",
        "\n",
        "    # Training\n",
        "    train_loss = train_epoch(model, optimizer, criterion, train_heads, train_tails, train_relations, node_emb)\n",
        "\n",
        "    # Evaluation on Validation Set\n",
        "    val_loss, val_accuracy = evaluate(model, criterion, valid_heads, valid_tails, valid_relations, node_emb)\n",
        "\n",
        "    # Early Stopping and Best Model Tracking\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        # Evaluate on Test Set\n",
        "        test_loss, test_accuracy = evaluate(model, criterion, test_heads, test_tails, test_relations, node_emb)\n",
        "        best_test_accuracy = test_accuracy\n",
        "\n",
        "\n",
        "    # Print Epoch Results\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_accuracy:.4f}, Best Test Acc={best_test_accuracy:.4f}, Curvature: {model.gcn.manifold.c.item():.6f}\")\n",
        "\n",
        "    curvatures.append(model.gcn.manifold.c.item())\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "print(\"\\nTraining Completed.\")\n",
        "print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
        "print(f\"Corresponding Test Accuracy: {best_test_accuracy:.4f}\")\n",
        "print(f\"Curvature: {model.gcn.manifold.c.item():.6f}\")\n",
        "\n",
        "##############################################\n",
        "# Inference and Evaluation\n",
        "##############################################\n",
        "\n",
        "def predict_relation(model, head_idx, tail_idx, node_emb):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        scores = model.classify(head_idx, tail_idx, node_emb)  # [batch_size, num_relations]\n",
        "        predicted_relations = torch.argmax(scores, dim=1)\n",
        "    return predicted_relations\n",
        "\n",
        "def evaluate_full(model, criterion, heads, tails, relations, node_emb):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        scores = model.classify(heads, tails, node_emb)  # [batch_size, num_relations]\n",
        "        loss = criterion(scores, relations).item()\n",
        "        preds = torch.argmax(scores, dim=1)\n",
        "        correct = (preds == relations).sum().item()\n",
        "        total = relations.size(0)\n",
        "        accuracy = correct / total\n",
        "    return loss, accuracy\n",
        "\n",
        "# Example Inference\n",
        "sample_head = train_triples[0][0]\n",
        "sample_tail = train_triples[0][2]\n",
        "sample_relation = train_triples[0][1]\n",
        "\n",
        "predicted_relation = predict_relation(model, torch.tensor([sample_head], device=device),\n",
        "                                      torch.tensor([sample_tail], device=device),\n",
        "                                      node_emb)\n",
        "\n",
        "print(\"\\nSample Inference:\")\n",
        "print(f\"Head Entity ID: {sample_head}, Tail Entity ID: {sample_tail}\")\n",
        "print(f\"Actual Relation ID: {sample_relation}, Predicted Relation ID: {predicted_relation.item()}\")"
      ],
      "metadata": {
        "id": "bYRe5inyIklo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################################\n",
        "# PLOTTING\n",
        "##########################################################\n",
        "import matplotlib.pyplot as plt\n",
        "epochs = list(range(1, num_epochs + 1))\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Plot training loss and test accuracy on the primary y-axis\n",
        "ax1.plot(epochs, train_losses, label=\"Train Loss\", color=\"blue\", linestyle=\"-\")\n",
        "ax1.plot(epochs, test_accuracies, label=\"Best Test Accuracy\", color=\"red\", linestyle=\"--\")\n",
        "ax1.set_xlabel(\"Epochs\")\n",
        "ax1.set_ylabel(\"Train Loss / Best Test Accuracy\", color=\"black\")\n",
        "ax1.tick_params(axis=\"y\", labelcolor=\"black\")\n",
        "ax1.legend(loc=\"upper left\")\n",
        "\n",
        "\n",
        "# Add a horizontal line at y=1\n",
        "ax1.axhline(y=1, color=\"lightcoral\", linestyle=\"--\", linewidth=1, label=\"y=1\")\n",
        "\n",
        "# Add a horizontal line at y=0\n",
        "ax1.axhline(y=0, color=\"grey\", linestyle=\"--\", linewidth=1, label=\"y=1\")\n",
        "\n",
        "# Plot curvature on a secondary y-axis\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(epochs, curvatures, label=\"Curvature\", color=\"green\", linestyle=\":\")\n",
        "ax2.set_ylabel(\"Curvature\", color=\"green\")\n",
        "ax2.tick_params(axis=\"y\", labelcolor=\"green\")\n",
        "ax2.legend(loc=\"upper right\")\n",
        "\n",
        "# Add title and grid\n",
        "plt.title(\"Training Progress: Train Loss, Best Test Accuracy, and Curvature\")\n",
        "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
        "plt.ylim(0, 1.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "##############################################\n",
        "# Inference and Evaluation\n",
        "##############################################\n",
        "\n",
        "def predict_relation(model, head_idx, tail_idx, node_emb):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        scores = model.classify(head_idx, tail_idx, node_emb)  # [batch_size, num_relations]\n",
        "        predicted_relations = torch.argmax(scores, dim=1)\n",
        "    return predicted_relations\n",
        "\n",
        "def evaluate_full(model, criterion, heads, tails, relations, node_emb):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        scores = model.classify(heads, tails, node_emb)  # [batch_size, num_relations]\n",
        "        loss = criterion(scores, relations).item()\n",
        "        preds = torch.argmax(scores, dim=1)\n",
        "        correct = (preds == relations).sum().item()\n",
        "        total = relations.size(0)\n",
        "        accuracy = correct / total\n",
        "    return loss, accuracy\n",
        "\n",
        "# Example Inference\n",
        "sample_head = train_triples[0][0]\n",
        "sample_tail = train_triples[0][2]\n",
        "sample_relation = train_triples[0][1]\n",
        "\n",
        "predicted_relation = predict_relation(model, torch.tensor([sample_head], device=device),\n",
        "                                      torch.tensor([sample_tail], device=device),\n",
        "                                      node_emb)\n",
        "\n",
        "print(\"\\nSample Inference:\")\n",
        "print(f\"Head Entity ID: {sample_head}, Tail Entity ID: {sample_tail}\")\n",
        "print(f\"Actual Relation ID: {sample_relation}, Predicted Relation ID: {predicted_relation.item()}\")"
      ],
      "metadata": {
        "id": "4-fNFOk9LvdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EMBEDDING VISUALS\n",
        "# 12/13: 6:50 PM\n",
        "# valentina\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from tqdm import tqdm\n",
        "import geoopt\n",
        "\n",
        "# Assuming LinkPredictionModel and necessary imports are already defined\n",
        "# Let's initialize the model and other training components\n",
        "\n",
        "hidden_dim = 128  # Reduced hidden dimension\n",
        "num_epochs = 500\n",
        "best_val_accuracy = 0.0\n",
        "best_test_accuracy = 0.0\n",
        "\n",
        "# Initialize the model\n",
        "model = LinkPredictionModel(num_features=embedding_dim, hidden_dim=hidden_dim, num_relations=num_relations, c=1.0).to(device)\n",
        "\n",
        "# Optimizer setup (Riemannian mixed precision)\n",
        "optimizer = geoopt.optim.RiemannianAdam(model.parameters(), lr=1e-2)\n",
        "\n",
        "# Loss function\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Prepare data\n",
        "train_heads, train_tails, train_relations = prepare_data(train_triples)\n",
        "valid_heads, valid_tails, valid_relations = prepare_data(valid_triples)\n",
        "test_heads, test_tails, test_relations = prepare_data(test_triples)\n",
        "\n",
        "curvatures, train_losses, val_losses, val_accuracies, test_accuracies = [], [], [], [], []\n",
        "\n",
        "# Initialize PCA for visualization\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Training Loop\n",
        "print(\"\\nStarting Training...\\n\")\n",
        "for epoch in tqdm(range(1, num_epochs + 1)):\n",
        "    # Forward pass to get node embeddings in hyperbolic space\n",
        "    node_emb_hyperbolic = model(features, norm_adj)  # [num_nodes, hidden_dim] in hyperbolic space\n",
        "\n",
        "    # Training\n",
        "    train_loss = train_epoch(model, optimizer, criterion, train_heads, train_tails, train_relations, node_emb_hyperbolic)\n",
        "\n",
        "    # Evaluation on Validation Set\n",
        "    val_loss, val_accuracy = evaluate(model, criterion, valid_heads, valid_tails, valid_relations, node_emb_hyperbolic)\n",
        "\n",
        "    # Early Stopping and Best Model Tracking\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        # Evaluate on Test Set\n",
        "        test_loss, test_accuracy = evaluate(model, criterion, test_heads, test_tails, test_relations, node_emb_hyperbolic)\n",
        "        best_test_accuracy = test_accuracy\n",
        "\n",
        "    # Print Epoch Results\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_accuracy:.4f}, Best Test Acc={best_test_accuracy:.4f}, Curvature: {model.gcn.manifold.c.item():.6f}\")\n",
        "\n",
        "    # Save the curvature, losses, and accuracies\n",
        "    curvatures.append(model.gcn.manifold.c.item())\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "    # Visualize embeddings every 100 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        # Project the hyperbolic embeddings to Euclidean space using expmap0\n",
        "        node_emb_euclidean = model.gcn.manifold.expmap0(node_emb_hyperbolic)  # Map hyperbolic to Euclidean space\n",
        "\n",
        "        # Reduce the dimensionality of the embeddings to 2D for visualization\n",
        "        node_emb_2d = pca.fit_transform(node_emb_euclidean.cpu().detach().numpy())\n",
        "\n",
        "        # Create the scatter plot\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.scatter(node_emb_2d[:, 0], node_emb_2d[:, 1], alpha=0.7, edgecolors='k', s=10)\n",
        "        plt.title(f\"Embeddings at Epoch {epoch}\")\n",
        "        plt.xlabel(\"Dimension 1\")\n",
        "        plt.ylabel(\"Dimension 2\")\n",
        "        plt.xlim(-2, 2)  # Adjust as needed\n",
        "        plt.ylim(-2, 2)  # Adjust as needed\n",
        "        plt.show()\n",
        "\n",
        "print(\"\\nTraining Completed.\")\n",
        "print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
        "print(f\"Corresponding Test Accuracy: {best_test_accuracy:.4f}\")\n",
        "print(f\"Curvature: {model.gcn.manifold.c.item():.6f}\")\n"
      ],
      "metadata": {
        "id": "nExM3PPnXKwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DISEASE"
      ],
      "metadata": {
        "id": "94uJAVRx9Kpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "from torch import optim\n",
        "import geoopt\n",
        "from geoopt import ManifoldParameter\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from scipy.sparse import csr_matrix, diags\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import scipy.sparse as sp\n",
        "\n",
        "##############################################\n",
        "# Data Loading and Preprocessing\n",
        "##############################################\n",
        "\n",
        "def load_synthetic_data(dataset_str, use_feats, data_path):\n",
        "    object_to_idx = {}\n",
        "    idx_counter = 0\n",
        "    edges = []\n",
        "\n",
        "    # Read all edges from the CSV file\n",
        "    with open(os.path.join(data_path, f\"{dataset_str}.edges.csv\"), 'r') as f:\n",
        "        all_edges = f.readlines()\n",
        "\n",
        "    for line in all_edges:\n",
        "        n1, n2 = line.rstrip().split(',')\n",
        "\n",
        "        # Assign unique indices to nodes\n",
        "        if n1 in object_to_idx:\n",
        "            i = object_to_idx[n1]\n",
        "        else:\n",
        "            i = idx_counter\n",
        "            object_to_idx[n1] = i\n",
        "            idx_counter += 1\n",
        "\n",
        "        if n2 in object_to_idx:\n",
        "            j = object_to_idx[n2]\n",
        "        else:\n",
        "            j = idx_counter\n",
        "            object_to_idx[n2] = j\n",
        "            idx_counter += 1\n",
        "\n",
        "        edges.append((i, j))\n",
        "\n",
        "    num_nodes = len(object_to_idx)\n",
        "\n",
        "    # Initialize adjacency matrix using LIL format for efficient assignment\n",
        "    adj = sp.lil_matrix((num_nodes, num_nodes), dtype=np.float32)\n",
        "\n",
        "    for i, j in edges:\n",
        "        adj[i, j] = 1.0  # Undirected graph: set both (i, j) and (j, i)\n",
        "        adj[j, i] = 1.0\n",
        "\n",
        "    # Convert adjacency matrix to CSR format for efficient arithmetic operations\n",
        "    adj = adj.tocsr()\n",
        "\n",
        "    # Load features\n",
        "    if use_feats:\n",
        "        features = sp.load_npz(os.path.join(data_path, f\"{dataset_str}.feats.npz\")).astype(np.float32)\n",
        "    else:\n",
        "        features = sp.eye(adj.shape[0], dtype=np.float32)\n",
        "\n",
        "    # Load labels\n",
        "    labels = np.load(os.path.join(data_path, f\"{dataset_str}.labels.npy\"))\n",
        "\n",
        "    return adj, features, labels, object_to_idx\n",
        "\n",
        "\n",
        "# Define file paths\n",
        "data_path = ''  # Update this to your actual data path\n",
        "dataset_str = 'disease_lp'\n",
        "\n",
        "# Load data\n",
        "adj, features, labels, object_to_idx = load_synthetic_data(dataset_str, use_feats=True, data_path=data_path)\n",
        "\n",
        "num_nodes = adj.shape[0]\n",
        "num_features = features.shape[1]\n",
        "num_classes = len(np.unique(labels))  # Assuming labels are integer-encoded\n",
        "\n",
        "print(f\"Number of Nodes: {num_nodes}\")\n",
        "print(f\"Number of Features: {num_features}\")\n",
        "print(f\"Number of Classes: {num_classes}\")\n",
        "\n",
        "# Convert adjacency matrix to a sparse matrix and add self-loops\n",
        "adj = adj + diags([1e-5] * num_nodes)\n",
        "\n",
        "# Compute degree\n",
        "degrees = np.array(adj.sum(axis=1)).flatten()\n",
        "\n",
        "# Compute D^-0.5 (inverse square root of degree matrix)\n",
        "inv_sqrt_deg = np.power(degrees, -0.5)\n",
        "inv_sqrt_deg[np.isinf(inv_sqrt_deg)] = 0\n",
        "D_inv_sqrt = diags(inv_sqrt_deg)\n",
        "\n",
        "# Normalize adjacency matrix: D^-0.5 * A * D^-0.5\n",
        "norm_adj = D_inv_sqrt @ adj @ D_inv_sqrt\n",
        "\n",
        "# Convert back to a PyTorch tensor\n",
        "norm_adj = torch.tensor(norm_adj.toarray(), dtype=torch.float32)\n",
        "\n",
        "# Node Features: Load from the features matrix\n",
        "features = csr_matrix(features)\n",
        "features = torch.tensor(features.toarray(), dtype=torch.float32)\n",
        "\n",
        "# Move tensors to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "features, norm_adj = features.to(device), norm_adj.to(device)\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "PozeRcURTpkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import csr_matrix, diags\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class HyperbolicLinear(nn.Module):\n",
        "    \"\"\"\n",
        "    Hyperbolic linear layer using the Poincaré Ball model.\n",
        "    \"\"\"\n",
        "    def __init__(self, manifold, in_features, out_features, bias=True):\n",
        "        super(HyperbolicLinear, self).__init__()\n",
        "        self.manifold = manifold\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        # ManifoldParameter for weight in Euclidean space\n",
        "        self.weight = ManifoldParameter(torch.randn(out_features, in_features) * 0.01, manifold=geoopt.Euclidean())\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is on manifold. Map to tangent space at 0, apply linear, map back\n",
        "        x_tan = self.manifold.logmap0(x, dim=-1)\n",
        "        out = F.linear(x_tan, self.weight, self.bias)\n",
        "        out = self.manifold.expmap0(out, dim=-1)\n",
        "        return out\n",
        "\n",
        "class HyperbolicGCNLayer(nn.Module):\n",
        "    def __init__(self, manifold, in_features, out_features):\n",
        "        super(HyperbolicGCNLayer, self).__init__()\n",
        "        self.manifold = manifold\n",
        "        self.lin = HyperbolicLinear(manifold, in_features, out_features)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        # x on manifold\n",
        "        x_tan = self.manifold.logmap0(x, dim=-1)\n",
        "        x_agg_tan = adj @ x_tan  # Aggregate in tangent space\n",
        "        x_agg = self.manifold.expmap0(x_agg_tan, dim=-1)\n",
        "        return self.lin(x_agg)\n",
        "\n",
        "class HyperbolicGCN(nn.Module):\n",
        "    def __init__(self, num_features, hidden_dim, c=1.0):\n",
        "        super(HyperbolicGCN, self).__init__()\n",
        "        # Initialize the manifold inside the model to ensure its parameters are included\n",
        "        self.manifold = geoopt.PoincareBall(c=c, learnable=True)\n",
        "        self.layer1 = HyperbolicGCNLayer(self.manifold, num_features, hidden_dim)\n",
        "        self.layer2 = HyperbolicGCNLayer(self.manifold, hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = self.manifold.expmap0(x, dim=-1)  # Ensure x is on manifold\n",
        "        x = self.layer1(x, adj)\n",
        "        # Hyperbolic activation: apply tanh in tangent space\n",
        "        x_tan = self.manifold.logmap0(x, dim=-1)\n",
        "        x_tan = torch.tanh(x_tan)\n",
        "        x = self.manifold.expmap0(x_tan, dim=-1)\n",
        "        x = self.layer2(x, adj)\n",
        "        return x\n",
        "\n",
        "class LinkPredictionModel(nn.Module):\n",
        "    def __init__(self, num_features, hidden_dim, num_relations, c=1.0):\n",
        "        super(LinkPredictionModel, self).__init__()\n",
        "        self.gcn = HyperbolicGCN(num_features, hidden_dim, c=c)\n",
        "        # Define a classifier that takes concatenated head and tail embeddings\n",
        "        self.classifier = nn.Linear(hidden_dim * 2, num_relations)  # Multi-class classification\n",
        "        nn.init.xavier_normal_(self.classifier.weight)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        node_emb = self.gcn(x, adj)  # [num_nodes, hidden_dim]\n",
        "        return node_emb\n",
        "\n",
        "    def classify(self, head_idx, tail_idx, node_emb):\n",
        "        # Retrieve embeddings\n",
        "        head_emb = node_emb[head_idx]     # [batch_size, hidden_dim]\n",
        "        tail_emb = node_emb[tail_idx]     # [batch_size, hidden_dim]\n",
        "        # Concatenate head and tail embeddings\n",
        "        combined = torch.cat([head_emb, tail_emb], dim=1)  # [batch_size, hidden_dim * 2]\n",
        "        scores = self.classifier(combined)  # [batch_size, num_relations]\n",
        "        return scores\n",
        "\n",
        "##############################################\n",
        "# Training and Evaluation Utilities\n",
        "##############################################\n",
        "\n",
        "def prepare_data(triples):\n",
        "    \"\"\"\n",
        "    Prepare tensors for head, tail, and relation indices.\n",
        "    \"\"\"\n",
        "    triples = torch.tensor(triples, dtype=torch.long, device=device)\n",
        "    heads = triples[:, 0]\n",
        "    relations = triples[:, 1]\n",
        "    tails = triples[:, 2]\n",
        "    return heads, tails, relations\n",
        "\n",
        "def train_epoch(model, optimizer, criterion, heads, tails, relations, node_emb):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Get relation scores\n",
        "    scores = model.classify(heads, tails, node_emb)  # [batch_size, num_relations]\n",
        "\n",
        "    # Compute loss\n",
        "    loss = criterion(scores, relations)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "def evaluate(model, criterion, heads, tails, relations, node_emb):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        scores = model.classify(heads, tails, node_emb)  # [batch_size, num_relations]\n",
        "        loss = criterion(scores, relations).item()\n",
        "        preds = torch.argmax(scores, dim=1)\n",
        "        correct = (preds == relations).sum().item()\n",
        "        total = relations.size(0)\n",
        "        accuracy = correct / total\n",
        "    return loss, accuracy\n",
        "\n",
        "##############################################\n",
        "# Training Loop\n",
        "##############################################\n",
        "\n",
        "# Generate Train, Validation, and Test Splits for Link Prediction\n",
        "\n",
        "def generate_negative_edges(adj_matrix, num_neg_samples, excluded_edges=set()):\n",
        "    neg_edges = set()\n",
        "    while len(neg_edges) < num_neg_samples:\n",
        "        i = random.randint(0, num_nodes - 1)\n",
        "        j = random.randint(0, num_nodes - 1)\n",
        "        if i == j:\n",
        "            continue\n",
        "        if adj_matrix[i, j] == 0 and (i, j) not in excluded_edges and (j, i) not in excluded_edges:\n",
        "            neg_edges.add((i, j))\n",
        "    return list(neg_edges)\n",
        "\n",
        "# Extract existing edges\n",
        "existing_edges = set()\n",
        "adj_coo = adj.tocoo()\n",
        "for i, j in zip(adj_coo.row, adj_coo.col):\n",
        "    if i < j:  # To avoid duplicates in undirected graph\n",
        "        existing_edges.add((i, j))\n",
        "\n",
        "# Number of positive and negative samples\n",
        "num_positive = len(existing_edges)\n",
        "num_negative = num_positive  # Balance the dataset\n",
        "\n",
        "# Generate negative edges\n",
        "negative_edges = generate_negative_edges(adj, num_negative, excluded_edges=existing_edges)\n",
        "\n",
        "# Create labels: 1 for positive, 0 for negative\n",
        "positive_labels = np.ones(num_positive)\n",
        "negative_labels = np.zeros(num_negative)\n",
        "\n",
        "# Combine positive and negative edges\n",
        "all_edges = list(existing_edges) + negative_edges\n",
        "all_labels = np.concatenate([positive_labels, negative_labels])\n",
        "\n",
        "# Split into train, val, test\n",
        "train_edges, temp_edges, train_labels, temp_labels = train_test_split(all_edges, all_labels, test_size=0.3, random_state=42, stratify=all_labels)\n",
        "val_edges, test_edges, val_labels, test_labels = train_test_split(temp_edges, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels)\n",
        "\n",
        "# Prepare training, validation, and test data\n",
        "train_triples = [(h, r, t) for (h, t), r in zip(train_edges, train_labels)]\n",
        "val_triples = [(h, r, t) for (h, t), r in zip(val_edges, val_labels)]\n",
        "test_triples = [(h, r, t) for (h, t), r in zip(test_edges, test_labels)]\n",
        "\n",
        "train_heads, train_tails, train_relations = prepare_data(train_triples)\n",
        "val_heads, val_tails, val_relations = prepare_data(val_triples)\n",
        "test_heads, test_tails, test_relations = prepare_data(test_triples)\n",
        "\n",
        "# Initialize the model\n",
        "hidden_dim = 128\n",
        "num_relations = len(np.unique(train_labels))\n",
        "model = LinkPredictionModel(num_features=num_features, hidden_dim=hidden_dim, num_relations=num_relations, c=1).to(device)\n",
        "\n",
        "# Initialize input: ensure features are on the manifold\n",
        "with torch.no_grad():\n",
        "    x = model.gcn.manifold.expmap0(features, dim=-1)\n",
        "\n",
        "# Optimizer Choices:\n",
        "riemannian_adam = geoopt.optim.RiemannianAdam(model.parameters(), lr=1e-4)\n",
        "riemannian_sgd = geoopt.optim.RiemannianSGD(model.parameters(), lr=1e-2)\n",
        "riemannian_mixed_precision = MixedPrecisionRiemannianAdam(model.parameters(), lr=1e-2)\n",
        "riemannian_adagrad = RiemannianAdagrad(model.parameters(), lr=1e-2) # error currently\n",
        "\n",
        "# SET OPTIMIZER:\n",
        "optimizer = riemannian_adam\n",
        "\n",
        "# Define loss function for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 500\n",
        "best_val_accuracy = 0.0\n",
        "best_test_accuracy = 0.0\n",
        "\n",
        "curvatures, train_losses, val_losses, val_accuracies, test_accuracies = [], [], [], [], []\n",
        "\n",
        "print(\"\\nStarting Training...\\n\")\n",
        "for epoch in tqdm(range(1, num_epochs + 1)):\n",
        "    # Forward pass to get node embeddings\n",
        "    node_emb = model(features, norm_adj)  # [num_nodes, hidden_dim]\n",
        "\n",
        "    # Training\n",
        "    train_loss = train_epoch(model, optimizer, criterion, train_heads, train_tails, train_relations, node_emb)\n",
        "\n",
        "    # Evaluation on Validation Set\n",
        "    val_loss, val_accuracy = evaluate(model, criterion, val_heads, val_tails, val_relations, node_emb)\n",
        "\n",
        "    # Early Stopping and Best Model Tracking\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        # Evaluate on Test Set\n",
        "        test_loss, test_accuracy = evaluate(model, criterion, test_heads, test_tails, test_relations, node_emb)\n",
        "        best_test_accuracy = test_accuracy\n",
        "\n",
        "    # Print Epoch Results\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_accuracy:.4f}, Best Test Acc={best_test_accuracy:.4f}, Curvature: {model.gcn.manifold.c.item():.6f}\")\n",
        "\n",
        "    curvatures.append(model.gcn.manifold.c.item())\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "    # Visualize embeddings every 10 epochs (or at any frequency)\n",
        "    #if epoch % 1000 == 0:\n",
        "        #visualize_embeddings_in_poincare(node_emb, labels, model.gcn.manifold, epoch=epoch)\n",
        "\n",
        "print(\"\\nTraining Completed.\")\n",
        "print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
        "print(f\"Corresponding Test Accuracy: {best_test_accuracy:.4f}\")\n",
        "print(f\"Curvature: {model.gcn.manifold.c.item():.6f}\")"
      ],
      "metadata": {
        "id": "5rajqLlGTgk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################################\n",
        "# PLOTTING\n",
        "##########################################################\n",
        "import matplotlib.pyplot as plt\n",
        "epochs = list(range(1, num_epochs + 1))\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Plot training loss and test accuracy on the primary y-axis\n",
        "ax1.plot(epochs, train_losses, label=\"Train Loss\", color=\"blue\", linestyle=\"-\")\n",
        "ax1.plot(epochs, test_accuracies, label=\"Best Test Accuracy\", color=\"red\", linestyle=\"--\")\n",
        "ax1.set_xlabel(\"Epochs\")\n",
        "ax1.set_ylabel(\"Train Loss / Best Test Accuracy\", color=\"black\")\n",
        "ax1.tick_params(axis=\"y\", labelcolor=\"black\")\n",
        "ax1.legend(loc=\"upper left\")\n",
        "\n",
        "\n",
        "# Add a horizontal line at y=1\n",
        "ax1.axhline(y=1, color=\"lightcoral\", linestyle=\"--\", linewidth=1, label=\"y=1\")\n",
        "\n",
        "# Add a horizontal line at y=0\n",
        "ax1.axhline(y=0, color=\"grey\", linestyle=\"--\", linewidth=1, label=\"y=1\")\n",
        "\n",
        "# Plot curvature on a secondary y-axis\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(epochs, curvatures, label=\"Curvature\", color=\"green\", linestyle=\":\")\n",
        "ax2.set_ylabel(\"Curvature\", color=\"green\")\n",
        "ax2.tick_params(axis=\"y\", labelcolor=\"green\")\n",
        "ax2.legend(loc=\"upper right\")\n",
        "\n",
        "# Add title and grid\n",
        "plt.title(\"Training Progress: Train Loss, Best Test Accuracy, and Curvature\")\n",
        "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
        "plt.ylim(0, 1.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O7d5p-6sfDwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CORA and PUBMED"
      ],
      "metadata": {
        "id": "CHJjN8bQwfeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define file paths\n",
        "data_path = ''  # **Update this to your actual data path**\n",
        "dataset_str = 'pubmed'  # cora or pubmed\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pickle as pkl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "from torch import optim\n",
        "import geoopt\n",
        "from geoopt import ManifoldParameter\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from scipy.sparse import csr_matrix, diags, vstack\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import scipy.sparse as sp\n",
        "\n",
        "##############################################\n",
        "# Auxiliary Functions\n",
        "##############################################\n",
        "\n",
        "def parse_index_file(filename):\n",
        "    \"\"\"Parse index file to list of indices.\"\"\"\n",
        "    index = []\n",
        "    with open(filename, 'r') as f:\n",
        "        for line in f:\n",
        "            index.append(int(line.strip()))\n",
        "    return index\n",
        "\n",
        "##############################################\n",
        "# Data Loading and Preprocessing\n",
        "##############################################\n",
        "\n",
        "def load_citation_data(dataset_str, use_feats, data_path, split_seed=None):\n",
        "    \"\"\"\n",
        "    Load citation network dataset (Cora, Citeseer, Pubmed).\n",
        "\n",
        "    Parameters:\n",
        "    - dataset_str: Name of the dataset (e.g., 'cora')\n",
        "    - use_feats: Whether to use features or identity matrix\n",
        "    - data_path: Path to the dataset files\n",
        "    - split_seed: Seed for splitting data (optional)\n",
        "\n",
        "    Returns:\n",
        "    - adj: Adjacency matrix (scipy sparse CSR matrix)\n",
        "    - features: Feature matrix (scipy sparse CSR matrix)\n",
        "    - labels: Numpy array of labels\n",
        "    - idx_train: List of training indices\n",
        "    - idx_val: List of validation indices\n",
        "    - idx_test: List of test indices\n",
        "    \"\"\"\n",
        "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
        "    objects = []\n",
        "    for name in names:\n",
        "        file_path = os.path.join(data_path, f\"ind.{dataset_str}.{name}\")\n",
        "        with open(file_path, 'rb') as f:\n",
        "            if sys.version_info > (3, 0):\n",
        "                out = pkl.load(f, encoding='latin1')\n",
        "            else:\n",
        "                out = pkl.load(f)\n",
        "            objects.append(out)\n",
        "\n",
        "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
        "\n",
        "    test_idx_reorder = parse_index_file(os.path.join(data_path, f\"ind.{dataset_str}.test.index\"))\n",
        "    test_idx_range = np.sort(test_idx_reorder)\n",
        "\n",
        "    # Handle feature matrices\n",
        "    if isinstance(allx, sp.csr_matrix):\n",
        "        features = sp.vstack((allx, tx)).tolil()\n",
        "    else:\n",
        "        features = sp.vstack((allx, tx)).tolil()\n",
        "\n",
        "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
        "\n",
        "    # Handle labels\n",
        "    labels = np.vstack((ally, ty))\n",
        "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
        "    labels = np.argmax(labels, axis=1)\n",
        "\n",
        "    # Define train, validation, and test indices\n",
        "    idx_test = test_idx_range.tolist()\n",
        "    idx_train = list(range(len(y)))\n",
        "    idx_val = list(range(len(y), len(y) + 500))  # Adjust the validation size as needed\n",
        "\n",
        "    # Create adjacency matrix\n",
        "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
        "    if not use_feats:\n",
        "        features = sp.eye(adj.shape[0])\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test\n",
        "\n",
        "# Load data using load_citation_data\n",
        "adj, features, labels, idx_train, idx_val, idx_test = load_citation_data(\n",
        "    dataset_str=dataset_str,\n",
        "    use_feats=True,\n",
        "    data_path=data_path\n",
        ")\n",
        "\n",
        "num_nodes = adj.shape[0]\n",
        "num_features = features.shape[1]\n",
        "num_classes = len(np.unique(labels))  # Assuming labels are integer-encoded\n",
        "\n",
        "print(f\"Number of Nodes: {num_nodes}\")\n",
        "print(f\"Number of Features: {num_features}\")\n",
        "print(f\"Number of Classes: {num_classes}\")\n",
        "\n",
        "# Convert adjacency matrix to a sparse matrix and add self-loops\n",
        "adj = adj + diags([1e-5] * num_nodes)  # Small value to prevent singularity\n",
        "\n",
        "# Compute degree\n",
        "degrees = np.array(adj.sum(axis=1)).flatten()\n",
        "\n",
        "# Compute D^-0.5 (inverse square root of degree matrix)\n",
        "inv_sqrt_deg = np.power(degrees, -0.5)\n",
        "inv_sqrt_deg[np.isinf(inv_sqrt_deg)] = 0\n",
        "D_inv_sqrt = diags(inv_sqrt_deg)\n",
        "\n",
        "# Normalize adjacency matrix: D^-0.5 * A * D^-0.5\n",
        "norm_adj = D_inv_sqrt.dot(adj).dot(D_inv_sqrt)\n",
        "\n",
        "# Convert back to a PyTorch tensor\n",
        "norm_adj = torch.tensor(norm_adj.toarray(), dtype=torch.float32)\n",
        "\n",
        "# Node Features: Load from the features matrix\n",
        "features = features.tocsr()\n",
        "features = torch.tensor(features.toarray(), dtype=torch.float32)\n",
        "\n",
        "# Move tensors to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "features, norm_adj = features.to(device), norm_adj.to(device)\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Generate Train, Validation, and Test Splits for Link Prediction\n",
        "\n",
        "def generate_negative_edges(adj_matrix, num_neg_samples, excluded_edges=set()):\n",
        "    \"\"\"\n",
        "    Generate negative edges (non-existing edges) for link prediction.\n",
        "\n",
        "    Parameters:\n",
        "    - adj_matrix: scipy sparse matrix (CSR format)\n",
        "    - num_neg_samples: Number of negative samples to generate\n",
        "    - excluded_edges: Set of edges to exclude (existing edges)\n",
        "\n",
        "    Returns:\n",
        "    - List of tuples representing negative edges\n",
        "    \"\"\"\n",
        "    neg_edges = set()\n",
        "    while len(neg_edges) < num_neg_samples:\n",
        "        i = random.randint(0, num_nodes - 1)\n",
        "        j = random.randint(0, num_nodes - 1)\n",
        "        if i == j:\n",
        "            continue\n",
        "        if adj_matrix[i, j] == 0 and (i, j) not in excluded_edges and (j, i) not in excluded_edges:\n",
        "            neg_edges.add((i, j))\n",
        "    return list(neg_edges)\n",
        "\n",
        "# Extract existing edges\n",
        "existing_edges = set()\n",
        "adj_coo = adj.tocoo()\n",
        "for i, j in zip(adj_coo.row, adj_coo.col):\n",
        "    if i < j:  # To avoid duplicates in undirected graph\n",
        "        existing_edges.add((i, j))\n",
        "\n",
        "# Number of positive and negative samples\n",
        "num_positive = len(existing_edges)\n",
        "num_negative = num_positive  # Balance the dataset\n",
        "\n",
        "# Generate negative edges\n",
        "negative_edges = generate_negative_edges(adj, num_negative, excluded_edges=existing_edges)\n",
        "\n",
        "# Create labels: 1 for positive, 0 for negative\n",
        "positive_labels = np.ones(num_positive)\n",
        "negative_labels = np.zeros(num_negative)\n",
        "\n",
        "# Combine positive and negative edges\n",
        "all_edges = list(existing_edges) + negative_edges\n",
        "all_labels = np.concatenate([positive_labels, negative_labels])\n",
        "\n",
        "# Split into train, val, test\n",
        "train_edges, temp_edges, train_labels, temp_labels = train_test_split(\n",
        "    all_edges, all_labels, test_size=0.3, random_state=42, stratify=all_labels\n",
        ")\n",
        "val_edges, test_edges, val_labels, test_labels = train_test_split(\n",
        "    temp_edges, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
        ")\n",
        "\n",
        "print(f\"Train edges: {len(train_edges)}, Val edges: {len(val_edges)}, Test edges: {len(test_edges)}\")\n"
      ],
      "metadata": {
        "id": "MI0slIbV6yQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################\n",
        "# Training Loop\n",
        "##############################################\n",
        "\n",
        "def prepare_edge_data(edges, labels):\n",
        "    \"\"\"\n",
        "    Prepare tensors for head, tail, and labels.\n",
        "    \"\"\"\n",
        "    heads = torch.tensor([edge[0] for edge in edges], dtype=torch.long, device=device)\n",
        "    tails = torch.tensor([edge[1] for edge in edges], dtype=torch.long, device=device)\n",
        "    labels = torch.tensor(labels, dtype=torch.long, device=device)  # Use torch.long for CrossEntropyLoss\n",
        "    return heads, tails, labels\n",
        "\n",
        "# Prepare training, validation, and test data\n",
        "train_heads, train_tails, train_labels = prepare_edge_data(train_edges, train_labels)\n",
        "val_heads, val_tails, val_labels = prepare_edge_data(val_edges, val_labels)\n",
        "test_heads, test_tails, test_labels = prepare_edge_data(test_edges, test_labels)\n",
        "\n",
        "# Initialize the model\n",
        "hidden_dim = 128  # Reduced hidden dimension\n",
        "num_relations = len(np.unique(train_labels.cpu().numpy()))\n",
        "model = LinkPredictionModel(num_features=num_features, hidden_dim=hidden_dim, num_relations = num_relations, c=1).to(device)\n",
        "\n",
        "# Initialize input: ensure features are on the manifold\n",
        "with torch.no_grad():\n",
        "    x = model.gcn.manifold.expmap0(features, dim=-1)\n",
        "\n",
        "# Optimizer Choices:\n",
        "riemannian_adam = geoopt.optim.RiemannianAdam(model.parameters(), lr=1e-3)\n",
        "riemannian_sgd = geoopt.optim.RiemannianSGD(model.parameters(), lr=5e-2)\n",
        "riemannian_mixed_precision = MixedPrecisionRiemannianAdam(model.parameters(), lr=1e-2)\n",
        "riemannian_adagrad = RiemannianAdagrad(model.parameters(), lr=1e-2) # error currently\n",
        "\n",
        "# SET OPTIMIZER:\n",
        "optimizer = riemannian_mixed_precision\n",
        "\n",
        "# Define loss function for binary classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 500\n",
        "best_val_accuracy = 0.0\n",
        "best_test_accuracy = 0.0\n",
        "\n",
        "curvatures, train_losses, val_losses, val_accuracies, test_accuracies = [], [], [], [], []\n",
        "\n",
        "print(\"\\nStarting Training...\\n\")\n",
        "for epoch in tqdm(range(1, num_epochs + 1)):\n",
        "    # Forward pass to get node embeddings\n",
        "    node_emb = model(features, norm_adj)  # [num_nodes, hidden_dim]\n",
        "\n",
        "    # Training\n",
        "    train_loss = train_epoch(model, optimizer, criterion, train_heads, train_tails, train_labels, node_emb)\n",
        "\n",
        "    # Evaluation on Validation Set\n",
        "    val_loss, val_accuracy = evaluate(model, criterion, val_heads, val_tails, val_labels, node_emb)\n",
        "\n",
        "    # Early Stopping and Best Model Tracking\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        # Evaluate on Test Set\n",
        "        test_loss, test_accuracy = evaluate(model, criterion, test_heads, test_tails, test_labels, node_emb)\n",
        "        best_test_accuracy = test_accuracy\n",
        "\n",
        "    # Print Epoch Results\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_accuracy:.4f}, Best Test Acc={best_test_accuracy:.4f}, Curvature: {model.gcn.manifold.c.item():.6f}\")\n",
        "\n",
        "    curvatures.append(model.gcn.manifold.c.item())\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "print(\"\\nTraining Completed.\")\n",
        "print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
        "print(f\"Corresponding Test Accuracy: {best_test_accuracy:.4f}\")\n",
        "print(f\"Curvature: {model.gcn.manifold.c.item():.6f}\")\n"
      ],
      "metadata": {
        "id": "OHt21OD67DnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################################\n",
        "# PLOTTING\n",
        "##########################################################\n",
        "import matplotlib.pyplot as plt\n",
        "epochs = list(range(1, num_epochs + 1))\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Plot training loss and test accuracy on the primary y-axis\n",
        "ax1.plot(epochs, train_losses, label=\"Train Loss\", color=\"blue\", linestyle=\"-\")\n",
        "ax1.plot(epochs, test_accuracies, label=\"Best Test Accuracy\", color=\"red\", linestyle=\"--\")\n",
        "ax1.set_xlabel(\"Epochs\")\n",
        "ax1.set_ylabel(\"Train Loss / Best Test Accuracy\", color=\"black\")\n",
        "ax1.tick_params(axis=\"y\", labelcolor=\"black\")\n",
        "ax1.legend(loc=\"upper left\")\n",
        "\n",
        "\n",
        "# Add a horizontal line at y=1\n",
        "ax1.axhline(y=1, color=\"lightcoral\", linestyle=\"--\", linewidth=1, label=\"y=1\")\n",
        "\n",
        "# Add a horizontal line at y=0\n",
        "ax1.axhline(y=0, color=\"grey\", linestyle=\"--\", linewidth=1, label=\"y=1\")\n",
        "\n",
        "# Plot curvature on a secondary y-axis\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(epochs, curvatures, label=\"Curvature\", color=\"green\", linestyle=\":\")\n",
        "ax2.set_ylabel(\"Curvature\", color=\"green\")\n",
        "ax2.tick_params(axis=\"y\", labelcolor=\"green\")\n",
        "ax2.legend(loc=\"upper right\")\n",
        "\n",
        "# Add title and grid\n",
        "plt.title(\"Training Progress: Train Loss, Best Test Accuracy, and Curvature\")\n",
        "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
        "plt.ylim(0, 1.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Rc-fCvE-sfIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import torch\n",
        "\n",
        "def visualize_embeddings_in_poincare(node_emb, labels, manifold, epoch=None):\n",
        "    \"\"\"\n",
        "    Visualizes the node embeddings in the Poincaré disk using t-SNE for dimensionality reduction.\n",
        "    It maps the hyperbolic embeddings to Euclidean space, reduces them to 2D, and visualizes them.\n",
        "\n",
        "    Parameters:\n",
        "    - node_emb: Tensor of node embeddings from the model [num_nodes, hidden_dim]\n",
        "    - labels: Array of node labels [num_nodes]\n",
        "    - manifold: The manifold (e.g., PoincaréBall) used for the embeddings\n",
        "    - epoch: The current epoch (optional, used in title)\n",
        "    \"\"\"\n",
        "    # Map to Euclidean space using logmap\n",
        "    node_emb_euclidean = manifold.logmap0(node_emb, dim=-1)\n",
        "\n",
        "    # Reduce to 2D using t-SNE\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    node_emb_2d = tsne.fit_transform(node_emb_euclidean.detach().cpu().numpy())\n",
        "\n",
        "    # Normalize to fit within the unit disk for visualization\n",
        "    norm = np.linalg.norm(node_emb_2d, axis=1, keepdims=True)\n",
        "    node_emb_2d = node_emb_2d / norm  # Keep points within the unit disk\n",
        "\n",
        "    # Plot the embeddings\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.scatter(node_emb_2d[:, 0], node_emb_2d[:, 1], c=labels, cmap=\"viridis\", s=30, alpha=0.7)\n",
        "    plt.colorbar()  # Show color bar to indicate label classes\n",
        "\n",
        "    # Draw the boundary of the Poincaré disk (unit circle)\n",
        "    circle = plt.Circle((0, 0), 1, color='r', fill=False, linestyle='--', linewidth=2)\n",
        "    plt.gca().add_artist(circle)\n",
        "\n",
        "    # Set plot limits and aspect\n",
        "    plt.title(f\"Node Embeddings in the Poincaré Disk (Epoch {epoch})\" if epoch else \"Node Embeddings in the Poincaré Disk\")\n",
        "    plt.axis('equal')\n",
        "    plt.xlim(-1, 1)\n",
        "    plt.ylim(-1, 1)\n",
        "    plt.show()\n",
        "\n",
        "def hyperbolic_to_euclidean(embedding):\n",
        "    \"\"\"Project hyperbolic embeddings from the Poincaré disk to Euclidean space.\"\"\"\n",
        "    x, y = embedding[:, 0], embedding[:, 1]\n",
        "    norm = x**2 + y**2\n",
        "    x_euclidean = 2 * x / (1 + norm)\n",
        "    y_euclidean = 2 * y / (1 + norm)\n",
        "    return np.stack([x_euclidean, y_euclidean], axis=1)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import torch\n",
        "\n",
        "def visualize_embeddings_in_batches_no_normalization(node_emb, labels, manifold, batch_size=1024, epoch=None):\n",
        "    \"\"\"\n",
        "    Visualizes the node embeddings in 2D using t-SNE, processing the data in batches.\n",
        "    It maps the hyperbolic embeddings to Euclidean space, reduces them to 2D, and visualizes them.\n",
        "\n",
        "    Parameters:\n",
        "    - node_emb: Tensor of node embeddings from the model [num_nodes, hidden_dim]\n",
        "    - labels: Array of node labels [num_nodes]\n",
        "    - manifold: The manifold (e.g., PoincaréBall) used for the embeddings\n",
        "    - batch_size: The batch size for processing (default is 1024)\n",
        "    - epoch: The current epoch (optional, used in title)\n",
        "    \"\"\"\n",
        "    # Create lists to store batch results\n",
        "    batch_node_emb_2d = []\n",
        "    batch_labels = []\n",
        "\n",
        "    # Process the embeddings in batches\n",
        "    num_batches = (len(node_emb) + batch_size - 1) // batch_size  # Calculate the number of batches\n",
        "    for i in range(num_batches):\n",
        "        # Select the batch (slice the embeddings and labels)\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = min((i + 1) * batch_size, len(node_emb))\n",
        "\n",
        "        batch_emb = node_emb[start_idx:end_idx]\n",
        "        batch_lbl = labels[start_idx:end_idx]\n",
        "\n",
        "        # Map the embeddings to Euclidean space using logmap\n",
        "        batch_emb_euclidean = manifold.logmap0(batch_emb, dim=-1)\n",
        "\n",
        "        # Reduce the embeddings to 2D using t-SNE\n",
        "        tsne = TSNE(n_components=2, random_state=42)\n",
        "        batch_emb_2d = tsne.fit_transform(batch_emb_euclidean.detach().cpu().numpy())\n",
        "\n",
        "        # Skip normalization for now (visualize in original space)\n",
        "        batch_node_emb_2d.append(batch_emb_2d)\n",
        "        batch_labels.append(batch_lbl)\n",
        "\n",
        "    # Concatenate the results from all batches\n",
        "    node_emb_2d = np.concatenate(batch_node_emb_2d, axis=0)\n",
        "    labels = np.concatenate(batch_labels, axis=0)\n",
        "\n",
        "    # Plot the embeddings\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.scatter(node_emb_2d[:, 0], node_emb_2d[:, 1], c=labels, cmap=\"viridis\", s=30, alpha=0.7)\n",
        "    plt.colorbar()  # Show color bar to indicate label classes\n",
        "\n",
        "    # Set plot limits and aspect\n",
        "    plt.title(f\"Node Embeddings (Epoch {epoch})\" if epoch else \"Node Embeddings\")\n",
        "    plt.axis('equal')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "k2SEYnlOfg7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE COMPARISON"
      ],
      "metadata": {
        "id": "TU0CPmKLASTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "from torch import optim\n",
        "import geoopt\n",
        "from geoopt import ManifoldParameter\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from scipy.sparse import csr_matrix, diags\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import scipy.sparse as sp\n",
        "\n",
        "##############################################\n",
        "# Data Loading and Preprocessing\n",
        "##############################################\n",
        "\n",
        "def load_synthetic_data(dataset_str, use_feats, data_path):\n",
        "    object_to_idx = {}\n",
        "    idx_counter = 0\n",
        "    edges = []\n",
        "\n",
        "    # Read all edges from the CSV file\n",
        "    with open(os.path.join(data_path, f\"{dataset_str}.edges.csv\"), 'r') as f:\n",
        "        all_edges = f.readlines()\n",
        "\n",
        "    for line in all_edges:\n",
        "        n1, n2 = line.rstrip().split(',')\n",
        "\n",
        "        # Assign unique indices to nodes\n",
        "        if n1 in object_to_idx:\n",
        "            i = object_to_idx[n1]\n",
        "        else:\n",
        "            i = idx_counter\n",
        "            object_to_idx[n1] = i\n",
        "            idx_counter += 1\n",
        "\n",
        "        if n2 in object_to_idx:\n",
        "            j = object_to_idx[n2]\n",
        "        else:\n",
        "            j = idx_counter\n",
        "            object_to_idx[n2] = j\n",
        "            idx_counter += 1\n",
        "\n",
        "        edges.append((i, j))\n",
        "\n",
        "    num_nodes = len(object_to_idx)\n",
        "\n",
        "    # Initialize adjacency matrix using LIL format for efficient assignment\n",
        "    adj = sp.lil_matrix((num_nodes, num_nodes), dtype=np.float32)\n",
        "\n",
        "    for i, j in edges:\n",
        "        adj[i, j] = 1.0  # Undirected graph: set both (i, j) and (j, i)\n",
        "        adj[j, i] = 1.0\n",
        "\n",
        "    # Convert adjacency matrix to CSR format for efficient arithmetic operations\n",
        "    adj = adj.tocsr()\n",
        "\n",
        "    # Load features\n",
        "    if use_feats:\n",
        "        features = sp.load_npz(os.path.join(data_path, f\"{dataset_str}.feats.npz\")).astype(np.float32)\n",
        "    else:\n",
        "        features = sp.eye(adj.shape[0], dtype=np.float32)\n",
        "\n",
        "    # Load labels\n",
        "    labels = np.load(os.path.join(data_path, f\"{dataset_str}.labels.npy\"))\n",
        "\n",
        "    return adj, features, labels, object_to_idx\n",
        "\n",
        "\n",
        "# Define file paths\n",
        "data_path = ''  # Update this to your actual data path\n",
        "dataset_str = 'disease_lp'\n",
        "\n",
        "# Load data\n",
        "adj, features, labels, object_to_idx = load_synthetic_data(dataset_str, use_feats=True, data_path=data_path)\n",
        "\n",
        "num_nodes = adj.shape[0]\n",
        "num_features = features.shape[1]\n",
        "num_classes = len(np.unique(labels))  # Assuming labels are integer-encoded\n",
        "\n",
        "print(f\"Number of Nodes: {num_nodes}\")\n",
        "print(f\"Number of Features: {num_features}\")\n",
        "print(f\"Number of Classes: {num_classes}\")\n",
        "\n",
        "# Convert adjacency matrix to a sparse matrix and add self-loops\n",
        "adj = adj + diags([1e-5] * num_nodes)\n",
        "\n",
        "# Compute degree\n",
        "degrees = np.array(adj.sum(axis=1)).flatten()\n",
        "\n",
        "# Compute D^-0.5 (inverse square root of degree matrix)\n",
        "inv_sqrt_deg = np.power(degrees, -0.5)\n",
        "inv_sqrt_deg[np.isinf(inv_sqrt_deg)] = 0\n",
        "D_inv_sqrt = diags(inv_sqrt_deg)\n",
        "\n",
        "# Normalize adjacency matrix: D^-0.5 * A * D^-0.5\n",
        "norm_adj = D_inv_sqrt @ adj @ D_inv_sqrt\n",
        "\n",
        "# Convert back to a PyTorch tensor\n",
        "norm_adj = torch.tensor(norm_adj.toarray(), dtype=torch.float32)\n",
        "\n",
        "# Node Features: Load from the features matrix\n",
        "features = csr_matrix(features)\n",
        "features = torch.tensor(features.toarray(), dtype=torch.float32)\n",
        "\n",
        "# Move tensors to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "features, norm_adj = features.to(device), norm_adj.to(device)\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "###################################################################################\n",
        "# GCN DEFINITION\n",
        "###################################################################################\n",
        "\n",
        "# Define the GCN Baseline Model\n",
        "class GCNBaseline(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_relations):\n",
        "        super(GCNBaseline, self).__init__()\n",
        "        self.conv1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.conv2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.classifier = nn.Linear(hidden_dim * 2, num_relations)  # For link prediction\n",
        "\n",
        "    def forward(self, features, adj):\n",
        "        x = F.relu(self.conv1(features))\n",
        "        x = torch.matmul(adj, x)  # Graph convolution using adjacency matrix\n",
        "        x = F.relu(self.conv2(x))\n",
        "        return x\n",
        "\n",
        "    def classify(self, head_idx, tail_idx, node_emb):\n",
        "        head_emb = node_emb[head_idx]\n",
        "        tail_emb = node_emb[tail_idx]\n",
        "        score = self.classifier(torch.cat([head_emb, tail_emb], dim=1))\n",
        "        return score\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, optimizer, criterion, heads, tails, relations, node_emb):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Get predictions for head-tail pairs\n",
        "    scores = model.classify(heads, tails, node_emb)  # [batch_size, num_relations]\n",
        "\n",
        "    # Compute loss\n",
        "    loss = criterion(scores, relations)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "# Link Prediction Model for Hyperbolic Graph Convolution\n",
        "class LinkPredictionModel(nn.Module):\n",
        "    def __init__(self, num_nodes, embedding_dim, num_relations, manifold, c):\n",
        "        super(LinkPredictionModel, self).__init__()\n",
        "        self.gcn = GCNBaseline(embedding_dim, embedding_dim, num_relations)\n",
        "        self.relation_embeddings = nn.Embedding(num_relations, embedding_dim)\n",
        "        self.manifold = manifold\n",
        "\n",
        "    def forward(self, features, adj):\n",
        "        # Generate node embeddings\n",
        "        node_embeddings = self.gcn(features, adj)\n",
        "        return node_embeddings\n",
        "\n",
        "    def classify(self, head_idx, tail_idx, node_embeddings):\n",
        "        # Calculate scores for all relations\n",
        "        head_emb = node_embeddings[head_idx]\n",
        "        tail_emb = node_embeddings[tail_idx]\n",
        "        scores = torch.matmul(head_emb, self.relation_embeddings.weight.T)\n",
        "        scores += torch.matmul(tail_emb, self.relation_embeddings.weight.T)\n",
        "        return scores\n",
        "\n",
        "############################################################\n",
        "#  TRAINING FUNCTIONS\n",
        "############################################################\n",
        "\n",
        "def prepare_data(triples):\n",
        "    \"\"\"\n",
        "    Prepare tensors for head, tail, and relation indices.\n",
        "    \"\"\"\n",
        "    triples = torch.tensor(triples, dtype=torch.long, device=device)\n",
        "    heads = triples[:, 0]\n",
        "    relations = triples[:, 1]\n",
        "    tails = triples[:, 2]\n",
        "    return heads, tails, relations\n",
        "\n",
        "def train_epoch(model, optimizer, criterion, heads, tails, relations, node_emb):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Get relation scores\n",
        "    scores = model.classify(heads, tails, node_emb)  # [batch_size, num_relations]\n",
        "\n",
        "    # Compute loss\n",
        "    loss = criterion(scores, relations)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "def evaluate(model, criterion, heads, tails, relations, node_emb):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        scores = model.classify(heads, tails, node_emb)  # [batch_size, num_relations]\n",
        "        loss = criterion(scores, relations).item()\n",
        "        preds = torch.argmax(scores, dim=1)\n",
        "        correct = (preds == relations).sum().item()\n",
        "        total = relations.size(0)\n",
        "        accuracy = correct / total\n",
        "    return loss, accuracy\n",
        "\n",
        "def generate_negative_edges(adj_matrix, num_neg_samples, excluded_edges=set()):\n",
        "    neg_edges = set()\n",
        "    while len(neg_edges) < num_neg_samples:\n",
        "        i = random.randint(0, num_nodes - 1)\n",
        "        j = random.randint(0, num_nodes - 1)\n",
        "        if i == j:\n",
        "            continue\n",
        "        if adj_matrix[i, j] == 0 and (i, j) not in excluded_edges and (j, i) not in excluded_edges:\n",
        "            neg_edges.add((i, j))\n",
        "    return list(neg_edges)\n",
        "\n",
        "# Extract existing edges\n",
        "existing_edges = set()\n",
        "adj_coo = adj.tocoo()\n",
        "for i, j in zip(adj_coo.row, adj_coo.col):\n",
        "    if i < j:  # To avoid duplicates in undirected graph\n",
        "        existing_edges.add((i, j))\n",
        "\n",
        "# Number of positive and negative samples\n",
        "num_positive = len(existing_edges)\n",
        "num_negative = num_positive  # Balance the dataset\n",
        "\n",
        "# Generate negative edges\n",
        "negative_edges = generate_negative_edges(adj, num_negative, excluded_edges=existing_edges)\n",
        "\n",
        "# Create labels: 1 for positive, 0 for negative\n",
        "positive_labels = np.ones(num_positive)\n",
        "negative_labels = np.zeros(num_negative)\n",
        "\n",
        "# Combine positive and negative edges\n",
        "all_edges = list(existing_edges) + negative_edges\n",
        "all_labels = np.concatenate([positive_labels, negative_labels])\n",
        "\n",
        "# Split into train, val, test\n",
        "train_edges, temp_edges, train_labels, temp_labels = train_test_split(all_edges, all_labels, test_size=0.3, random_state=42, stratify=all_labels)\n",
        "val_edges, test_edges, val_labels, test_labels = train_test_split(temp_edges, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels)\n",
        "\n",
        "# Prepare training, validation, and test data\n",
        "train_triples = [(h, r, t) for (h, t), r in zip(train_edges, train_labels)]\n",
        "val_triples = [(h, r, t) for (h, t), r in zip(val_edges, val_labels)]\n",
        "test_triples = [(h, r, t) for (h, t), r in zip(test_edges, test_labels)]\n",
        "\n",
        "train_heads, train_tails, train_relations = prepare_data(train_triples)\n",
        "val_heads, val_tails, val_relations = prepare_data(val_triples)\n",
        "test_heads, test_tails, test_relations = prepare_data(test_triples)\n",
        "\n",
        "# Initialize the model\n",
        "hidden_dim = 128\n",
        "num_relations = len(np.unique(train_labels))\n",
        "model = LinkPredictionModel(num_features=num_features, hidden_dim=hidden_dim, num_relations=num_relations, c=1).to(device)\n",
        "\n",
        "# Initialize input: ensure features are on the manifold\n",
        "with torch.no_grad():\n",
        "    x = model.gcn.manifold.expmap0(features, dim=-1)\n",
        "\n",
        "# Optimizer Choices:\n",
        "riemannian_adam = geoopt.optim.RiemannianAdam(model.parameters(), lr=1e-4)\n",
        "riemannian_sgd = geoopt.optim.RiemannianSGD(model.parameters(), lr=1e-2)\n",
        "riemannian_mixed_precision = MixedPrecisionRiemannianAdam(model.parameters(), lr=1e-2)\n",
        "riemannian_adagrad = RiemannianAdagrad(model.parameters(), lr=1e-2) # error currently\n",
        "\n",
        "# SET OPTIMIZER:\n",
        "optimizer = riemannian_adam\n",
        "\n",
        "# Define loss function for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 500\n",
        "best_val_accuracy = 0.0\n",
        "best_test_accuracy = 0.0\n",
        "\n",
        "curvatures, train_losses, val_losses, val_accuracies, test_accuracies = [], [], [], [], []\n",
        "\n",
        "print(\"\\nStarting Training...\\n\")\n",
        "for epoch in tqdm(range(1, num_epochs + 1)):\n",
        "    # Forward pass to get node embeddings\n",
        "    node_emb = model(features, norm_adj)  # [num_nodes, hidden_dim]\n",
        "\n",
        "    # Training\n",
        "    train_loss = train_epoch(model, optimizer, criterion, train_heads, train_tails, train_relations, node_emb)\n",
        "\n",
        "    # Evaluation on Validation Set\n",
        "    val_loss, val_accuracy = evaluate(model, criterion, val_heads, val_tails, val_relations, node_emb)\n",
        "\n",
        "    # Early Stopping and Best Model Tracking\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        # Evaluate on Test Set\n",
        "        test_loss, test_accuracy = evaluate(model, criterion, test_heads, test_tails, test_relations, node_emb)\n",
        "        best_test_accuracy = test_accuracy\n",
        "\n",
        "    # Print Epoch Results\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_accuracy:.4f}, Best Test Acc={best_test_accuracy:.4f}, Curvature: {model.gcn.manifold.c.item():.6f}\")\n",
        "\n",
        "    curvatures.append(model.gcn.manifold.c.item())\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "    # Visualize embeddings every 10 epochs (or at any frequency)\n",
        "    #if epoch % 1000 == 0:\n",
        "        #visualize_embeddings_in_poincare(node_emb, labels, model.gcn.manifold, epoch=epoch)\n",
        "\n",
        "print(\"\\nTraining Completed.\")\n",
        "print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
        "print(f\"Corresponding Test Accuracy: {best_test_accuracy:.4f}\")\n",
        "print(f\"Curvature: {model.gcn.manifold.c.item():.6f}\")\n"
      ],
      "metadata": {
        "id": "8eX5eXESARpr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}